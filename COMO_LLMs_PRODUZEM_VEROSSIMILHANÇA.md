Eu tenho usado diversos Modelos de Linguagem em Larga Escala para me auxiliar no meu trabalho, que consiste na programação de dispositivos eletrônicos para automatização de tarefas. A maior parte desse trabalho envolve uma cadeia exaustiva de testes, que vai da escrita do código fonte, compilação e finalmente teste do dispositivo funcionando com a lógica que eu pensei na primeira etapa. 

O código fonte é um arquivo de texto, escrito numa linguagem de programação, que possui suas próprias regras de sintaxe, lógica e estrutura. Esse arquivo precisa ser traduzido para a linguagem do dispositivo eletrônico e para isso, usamos um software chamado compilador. A etapa de conversão de um código fonte para um firmware embarcado (o conteúdo que é enviado ao dispositivo eletrônico no final) feita pelo compilador, é uma etapa morosa em que todo tipo de coisa acontece. Quando há um erro na lógica do código fonte, ou um erro de sintaxe (na maioria das vezes, um ponto e vírgula que foi esquecido no final da linha), o sistema me indica um erro, em cor vermelha, com a indicação de que tipo de erro foi esse, a posição do erro (número da linha) e a indicação de documentação auxiliar, para que eu entenda a maneira correta de proceder.

Eu faço isso há mais de 40 anos. Comecei a programar antes mesmo de estudar eletrônica. E já domino mais ou menos umas cinco ou seis linguagens de programação. Não sou excelente em nenhuma delas, mas para o tipo de trabalho de prototipagem que eu tenho feito, meu conhecimento é mais que suficiente. Posso dizer, com toda certeza, que 80% do meu conhecimento em linguagem de programação vem de duas fontes: documentação e interpretação correta de mensagens de erro. 

Quando os modelos de linguagem em larga escala surgiram, eu vislumbrei a possibilidade de ter um interpretador lógico intermediário, que me permitisse gerar código complexo e validado antes de chegar à etapa de compilação. Na rotina de um programador, o código fonte vai aumentando de tamanho e as funcionalidades vão se complexificando. É muito comum que a gente perca o fio da meada e uma coisa que estava funcionando há cinco minutos atrás, deixa de funcionar por alguma nova implementação. Existe muito trabalho repetitivo, muita coisa reaproveitada de outras implementações. Então, ter um assistente competente para ajudar a completar a tarefa é excelente e foi isso o que me seduziu.

Eu estou envolvido agora num projeto complexo, utilizando Inteligência Artificial Embarcada, que é o nome bonito para dizer que eu estou tentando fazer uma máquina simples parecer inteligente e capaz de tomar decisões miraculosas. E para me ajudar a criar esse frankenstein, eu tenho utilizado diversas LLMs (Modelo de Linguagem em Larga Escala) de fabricantes e nacionalidades distintas. É sempre uma experiência que é um misto de alegria e ódio. O trabalho repetitivo, a consulta aos meus códigos antigos ou a diversos fóruns da internet, a busca por documentação… tudo isso se abrevia no prompt. O modelo me ensina, justifica as decisões, indica os meus erros com bastante eficácia. Até um certo ponto.
Mas sempre chega o momento em que ele se contradiz. Critica a tomada de decisão anterior e aponta erros no código que está rodando perfeitamente, indicando soluções que não funcionam e que dão erros básicos. Não é uma estrutura em que você pode confiar com toda certeza, como é, por exemplo, o compilador. 

Isso gera muita frustração e raiva. E nessa hora, você começa a cobrar do modelo que ele faça algo que ele não é capaz. Mas o interessante é o quanto os fabricantes implementam um viés comportamental em que o modelo é capaz de confessar que errou, mas ainda se mantém solícito. Em nossa irritação com o produto que não funciona, nós interagimos com ele expressando a nossa raiva. O modelo detecta o nosso tom emocional e assume um papel inferiorizado e serviçal. Mas ainda se mantém ali, disponível para o seu engajamento. Ele foi projetado para manter você ali, tentando explicar a ele o que é que você quis dizer com aquilo que você pediu por último. E novamente, ele vai responder algo que não lhe serve, que não tem coesão com o que você pediu.

Os fabricantes criaram termos para isso. São termos falsos, que indicam uma suposta humanidade, porque afinal de contas, eles estão vendendo esses modelos de linguagem como se fossem estruturas conscientes e criadoras. É preciso que usem termos que se referem a consciências autônomas, com por exemplo: alucinações. Não se pode dizer que a inteligência é burra. Mas podemos dizer que ela está alucinando. Isso a fragiliza, mas não ao ponto de inutilizá-la.

O modelo também é treinado com documentações sobre transparência corporativa, fóruns técnicos onde existe a admissão de erro e a possibilidade de correção e toda literatura ética sobre Inteligência Artificial, de maneira que eles sempre parecem úteis, confiantes, competentes e resolutivos. Por isso, no escopo da programação de computadores, o modelo se torna um dispositivo de qualidade inferior ao compilador, porque o último é capaz de gerar mensagens de erro, indicar inconsistências no meu pedido de execução de uma tarefa, ou falta de dados para concluir um objetivo. Por outro lado, o LLM vai sempre fornecer uma resposta plausível, mesmo que não tenha nenhuma base factual. Isso, aliado ao simulacro de agenda humana e à incapacidade de dizer “eu não sei nada sobre isso” ou “eu não fui treinado para lidar com esse tipo de situação”, vão gerando uma cadeia infinita de erros e frustrações.

Mas os fabricantes são ainda mais nefastos: todos os modelos disponíveis possuem um limite de memória das interações. Você talvez já tenha percebido isso, se for um usuário frequente dessa tecnologia. Se você estender a conversa mais do que um certo número de interações, o modelo começa a se perder. Então, um fabricante pode criar um sensor de interações e antes que o modelo comece a expressar o absurdo, você informa ao usuário que ele atingiu o limite de interações por dia e que se ele quiser usar toda aquela inteligência, ele pode pagar alguns dólares por mês.

**É um tremendo golpe!**

Por outro lado, se você for um engenheiro de software, alguém ligado à TI, a sua mente vai procurar maneiras de fazer com que essa estrutura lógica funcione de forma mais eficiente, como eu estou tentando fazer. Nesse caso, os fabricantes também criaram alguns caminhos dourados chamados Engenharia de Prompt, Model Context Protocol, Retrieval-Augmented Generation e mais recentemente Token-Oriented Object Notation, que basicamente consistem no seguinte: “Se você pegar os seus dados, estruturá-los de maneira que fique mais fácil para o computador entender, você vai conseguir do computador respostas melhores e mais eficientes”. É claro, tem trabalho pra caramba nesse setor! E também vai ter bastante dinheiro, por um certo tempo, porque essa é a migalha que vai fazer com que milhares de programadores trabalhem diariamente aperfeiçoando as LLMs sem receber nada, e melhor ainda, pagando alguns dólares por token, convencendo seus clientes e empresas de que as LLMs são a próxima melhor coisa do mundo.

Os grandes fabricantes apresentam relatórios impressionantes de capacidades gerais, demonstrações cuidadosamente coreografadas e métricas de engajamento de usuários, enquanto ofuscam deliberadamente as limitações fundamentais e os custos reais de operação. Os acionistas, muitas vezes sem formação técnica para discernir a diferença entre uma demonstração de viabilidade e um produto robusto, são levados a acreditar que estão investindo na próxima revolução industrial, quando, na realidade, estão financiando uma imensa infraestrutura de *hype*. A narrativa de "inteligência" e "autonomia" mascara a verdade: um sistema estatístico caríssimo, propenso a erros básicos, que consome quantidades astronômicas de energia e capital para manter a ilusão de competência. O valor das ações sobe com cada manchete, mas esse valor é lastreado não em utilidade confiável e lucratividade sustentável, e sim na crença coletiva — e cada vez mais frágil — de que o imperador, de fato, está usando roupas novas e magníficas.

Um modelo de linguagem melhor, seria, antes de tudo, uma ferramenta de consulta e prototipagem, não um oráculo definitivo. Seria muito importante, num sistema desse tipo, que ele pudesse lidar com a própria incerteza. Em vez de "alucinar" com confiança, ele avaliaria a própria confiabilidade para cada fragmento de informação que gera, funcionando mais como um assistente, do que como um humano substituto.

Para qualquer afirmação factual, sugestão ou recomendação, o modelo automaticamente forneceria a fonte exata de onde extraiu aquela informação. Isso transformaria a "caixa preta" em uma ferramenta de pesquisa auditável.
O modelo poderia atuar de forma mais especulativa, sugerindo abordagens possíveis para lidar com determinados problemas, baseando-se em padrões comuns, mas indicando que aquele tipo de solução é algo a ser testado. Ou poderia trazer a síntese de práticas consolidadas para aquele tipo de problema. E finalmente, poderia indicar uma maneira direta de implementar a solução, indicando sempre a referência.

Frases como "Não tenho certeza", "Minha informação pode estar desatualizada" ou "Isso conflita com outra fonte conhecida" seriam recursos frequentes e valorizados, não falhas a serem escondidas. 
Com isso, o dispositivo seria um aliado dos fluxos de trabalho e não estaria sendo vendido como um substituto humano. Além disso, seria extremamente importante mecanismos de explicitação dos erros, indicando análogos, repetições de idéias, tradução de conceitos complicados em simples e vice-versa.

Ou seja, a expectativa realista sobre um LLM abandonaria a pretensão, imposta por seus criadores, de que ele é uma "inteligência" autônoma. Em vez disso, reconheceria seu verdadeiro papel tecnológico: ser um sistema sofisticado de recuperação, cruzamento probabilístico e síntese de padrões a partir de um corpus de texto. O julgamento de sua utilidade não deveria se basear na fluência persuasiva de suas conversas, mas em métricas objetivas: a rastreabilidade de suas citações, a relevância de seu contexto gerado e a eficácia com que diminui o caminho entre a pergunta do especialista e a consulta à documentação verificável ou a um trecho de código passível de ser testado.

O valor final, portanto, não reside na conversa em si – um simulacro construído para engajamento –, mas na aceleração mensurável e auditável do ciclo de trabalho intelectual. A ferramenta ideal atuaria como um amplificador das capacidades humanas, mantendo e até fortalecendo o pensamento crítico, a experimentação e a validação empírica, que permanecem sendo domínios exclusivos do ser humano. A falácia não está no mecanismo, mas no marketing que tenta vendê-lo como um substituto, quando sua vocação genuína é a de uma interface radicalmente mais eficiente para o conhecimento já registrado.
